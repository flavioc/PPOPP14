First, beyond the pro forma thanks, we want to express our sincere
appreciation for the high quality reviews.  The paper is rough
particularly with respect to the evaluation.  We are probably trying
to accomplish too much by simultaneously introducing logic
programming, linear logic (LL), and coordination.  We appreciate the
guidance in changing the presentation to make it clearer to the
reader.

related work (#2,#3,#4): 

	We will expand the comparison to other
	functional/declarative/coordination languages.

Why splashBP (#1,#2):
Evaluation against other languages (all): 

	Only direct comparison is to graphlab using C++ (specifically
	splashBP).  LM is 3-4x slower than C++.  We will clear up the
	description of splashBP and cite graphlab.  All our sources
	are available on github.

	We did not go into more detail because LM is currently
	interpreted by a very UNoptimized interpreter.  We are
	currently working on this, but felt that the ideas of LL,
	coordination, and provability were interesting as long as we
	could show basic scalability.  

Suitable algorithms (#1): 

	LM is targeted towards graph based algorithms, particularly
	with irregular parellelism.

Pruning & improvement (#1,#3): 

	For heat transfer program, the number of applied rules
	decreases by 50% when using coordination.

Advantages of LL (all): 

	Both performance and programming ease:
	1-To our knowledge LM is the only example of using LL to solve
	real world problems,
	2-It supports state manipulation while still remaining
	declarative. (compare to prolog's cut/assert/retract etc.)
	3-Often uses less memory, 
	4-programs become shorter, 
	5-gives programmer sense of control while still being declarative.  

Ease of programming (#2): 

	Comes from 1-LL and 2-allowing coordination to be specified in
	the same language.

Scalability (#3): 

	We will be clearer in the paper.  The queues are all local.
	The only issue comes in detecting termination.  (As an aside:
	We recently have LM running under MPI with similar scaling
	results.)

superlinear speedup (#1,#3,#4): 

	We will explain this better.  In short, the best sequential
	program would be very hard to write.  The parallel code
	updates the belief's of each node non-determinisitically
	(between 1 and 4 updates before recalculation) and thus often
	get faster convergence than the serial which does exactly 2
	every time.

priority (#3): 

	If the order is descending, the priority will become negative.

database prefix (#3): 

	 The common prefix is for facts with same arguments: e.g.,
	 f(1,2,4) and f(1,2,5) share 1 and 2.

rules without heads (#4): 

   They don't have a name but they can be expressed as follows
	a(X),b(X) -o 1.

new axioms (#4): 

	Yes.  
	  axiom(A, ...). // for all nodes
	  axiom(@0, â€¦). // just for node 0

hiding parallelism (#4): 

	We believe that our approach of allowing the programmer to
	guide the parallelism through the first class coordination
	directives gives the programmer the best of both worlds: let
	the compiler do as much as possible and then allow the
	programmer to optimize as needed WITHOUT requiring the
	algorithm to be obscured by the parallelization process.

analysis (#4): 

	 In previous work we have shown that we can prove deadlock and
	 livelock freedom for a non-trival parallel program.

