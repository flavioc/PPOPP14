\lang is a \emph{forward chaining} logic programming language in the style of Datalog~\cite{Ullman:1990:PDK:533142}. The program is defined as a set of axioms (facts that are initially true) and a set of \emph{derivation rules}. Initially, we populate the \emph{database of facts} with the axioms and then we determine which derivation rules can be applied by using our current database. Once a rule is applied, we derive new facts, which are then added to the database
This process is repeated until we reach \emph{quiescence}, that is, when we can no longer derive new facts.

Each fact is an association between a \emph{predicate} and a tuple of values. A predicate is a pair with a name and a tuple of types (the argument types). \lang rules are type-checked using the predicate declarations in the header of the program. \lang has a simple type system that includes the following types: \emph{node}, \emph{int}, \emph{float}, \emph{string}, \emph{bool} and \emph{list X}, where \emph{X} is any of the previous 5 types.

Every derivation rule can be decomposed into one \emph{body} and one \emph{head}.
The body contains the prerequisites needed (including facts and constraints) to derive the expression in the head.
An example of a rule can be seen in line 11 in Fig.~\ref{code:visit}.
The body of this rule is \texttt{visit(A), visited(A)} and the head is \texttt{visited(A)}.
Rules without bodies are allowed in \lang and they are called axioms (line 15 in Fig.~\ref{code:visit}). Rules without heads are also allowed.

\subsection{Distribution}

The first argument of every predicate must be typed as a \emph{node}. The node represents a vertex in the \emph{graph structure}.
Essentially, a program can be seen as a graph data structure where processors do computation at the node level. Computation is
parallelized by processing many nodes concurrently. The edges of this graph are defined through a special class of facts called \emph{structural facts}
and allow transmission of facts between nodes.

For distribution and data partitioning purposes, derivation rules are constrained by the expressions that can be written in the body.
The body of every rule can only refer to facts in the same node.
However, the expressions in the head may refer to other nodes, as long as those nodes are instantiated in the body of the rule.
The database of the program is then partitioned by the first argument of each fact. This is possible since the rules of the
program only make use of facts from a single node. The facts of each node represent both the state of the computation and data.
We drew some inspiration from the Linda system~\cite{1663305}, where the tuple space contains the data and is used by the processors
to communicate and do computation.
This differs from imperative languages, since in those languages data and computation are two separate entities.

\subsection{Linearity}

Another big departure from Datalog-like languages is the use of linear logic~\cite{Girard95logic:its}. Traditional forward-chaining logic programming languages make only use of classical logic, in which derived facts are true forever. Many ad-hoc extensions~\cite{Liu98extendingdatalog,Ludascher95alogical} have been devised in the past to support state updates in Datalog, but most are extra-logical which makes it harder to reason about programs.

With linear logic, we define two classes of facts: \emph{persistent facts} and \emph{linear facts}. Persistent facts, once derived, are true until the end of execution and work as in classical logic. Linear facts are seen as resources and when used in a derivation, they are deleted. This allows programs to transparently express state updates while keeping the full benefits of classical logic.

We use a small subset of the original linear logic proof system with some extensions to improve
the expressiveness of the language. We summarize the connectives used in Table~\ref{table:linear}.

\begin{table*}
   \begin{center}
\resizebox{17cm}{!}{
    \begin{tabular}{ | l | l | l | l | l |}
    \hline
    Connective & Place & \lang Syntax & \lang Example & Description \\ \hline \hline
    $\emph{fact}(e_1, ...,e_n)$ & Body or Head & $fact(A, B, ..., Z)$ & \texttt{path(A, P)} & Linear facts. \\ \hline
    $\bang \emph{fact}(e_1, ..., e_n)$ & Body or Head & $\bang fact(A, B, ..., Z)$ & \texttt{$\bang$edge(X, Y, W)} & Persistent facts. \\ \hline
    $1$ & Head & $1$ & \texttt{1} & Represents rules with an empty head. \\ \hline
    $A \otimes B$ & Body and Head & $A, B$ & \texttt{path(A, P), edge(A, B, W)} & Connect two expressions. \\ \hline
    $\forall x. A$ & Rule & Please see $A \lolli B$ & \texttt{path(A, B) $\lolli$ reachable(A, B)} & To represent variables defined inside the rule. \\ \hline
    $\exists x. A$ & Head & $exists \; A. (..., \emph{fact}(A, ...), ...)$ & \texttt{exists A.(path(A, P))} & Instantiates new node variables. \\ \hline
    $A \lolli B$ & Rule & $A \lolli B$ & \texttt{path(A, B) $\lolli$ reachable(A, B)} & $\lolli$ means "linearly implies". \\& & & & $A$ is the body and $B$ is the head. \\ \hline
    $\m{def} A. B$ & Head & $\{X, ..., Z | A | B\}$ & \texttt{\{B | !edge(A, B) | visit(B)\}} & Extension called definitions.\\ & & & & Used for comprehensions and aggregates. \\ \hline
    \end{tabular}
}
\end{center}
\caption{Connectives from Linear Logic used in \lang.}
\label{table:linear}
\end{table*}

\subsection{Example}

In Fig.~\ref{code:visit} we present a complete \lang program for doing a visit of all nodes
in a graph, starting at node $@1$. We first declare all the predicates (lines 1-4). Note that \texttt{route} predicates are classified as structural predicates and that non \texttt{linear} facts are classified as persistent so the \texttt{edge} predicate is persistent while everything else is linear.
Next, we declare our program rules (lines 8-11), followed by the program axioms (lines 15-18).
Node $@1$ starts with the \texttt{visit(@1)} fact.

\begin{figure}[h!]
\small\begin{Verbatim}[numbers=left]
type route edge(node, node).
type linear visit(node).
type linear unvisited(node).
type linear visited(node).

// the program rules

visit(A), unvisited(A) -o
   visited(A), {B | !edge(A, B) | visit(B)}.

visit(A), visited(A) -o visited(A).

// axioms: the input data

!edge(@1, @2). !edge(@2, @3). !edge(@1, @4). !edge(@2, @4).
unvisited(@1). unvisited(@2). unvisited(@3). unvisited(@4).

visit(@1).
\end{Verbatim}
  \caption{Visit program.}
  \label{code:visit}
\end{figure}
\normalsize

The first rule of the program (lines 8-9) is fired if the node has a \texttt{visit} and a \texttt{unvisited} fact. When fired, we first derive \texttt{visited} to mark node as "visited" and use a
comprehension to go through all the edge facts and derive \texttt{visit} in every one of them.
This forces those nodes to be visited also. The second rule (line 11) is fired when the node is already
visited more than once: we keep the \texttt{visited} fact and delete \texttt{visit}.

Note that it is very easy to prove properties about this program. First, a node is either
\texttt{visited} or \texttt{unvisited}. Also, once \texttt{visited} it no longer changes to
\texttt{unvisited}.

\subsection{Sensing and acting}

Apart from structural facts, \lang also employs \emph{computation facts}, \emph{sensing facts}
and \emph{action facts}.
Computation facts are regular facts used to represent the program state.
Sensing facts are facts about the current state of the runtime system, such as the placement
of nodes in the CPU and scheduling information. In Meld, sensing facts
were used to get information about the outside world, like temperature, touch, neighborhood status,
etc; while action facts were used by the robots to perform actions.

Action facts are used to perform two main functions. One is for doing I/O or changing things
in the user interface. For example, when we want to change the color of nodes or the label
of edges, we just derive a new action fact and the action is performed in the interface.
Another use of action facts is to change the order things are evaluated in the runtime system.
It is possible to give hints to the virtual machine in order to prioritize the computation of
some nodes.
Action facts are linear facts which are consumed when the corresponding action is performed.

With sensing facts and action facts, we can write "meta-rules" that will sense the state of the runtime system
and then apply decisions in order to improve execution speed. In some situations, such set of rules can be
added to the program without any modifications to the original program.

\subsection{Rules}

Rules are written as $A \lolli B$, where $A$ is the body of the rule and $B$ is the head.
The body of the rule contains \emph{fact expressions} and constraints.
Fact expressions are template facts that instantiate variables (from facts in the database)
such as \texttt{visit(A)} in line 11 in Fig.~\ref{code:visit}. Variables can be used again in the body for matching and
also in the head when instantiating facts. Constraints are boolean expressions that must
be true in order for the rule to be fired. Constraints use variables from fact expressions and are built using a small functional language that includes mathematical operations, boolean operations, external functions and literal values.

When a rule body is instantiated using facts from the database, facts are picked non-deterministically.
While our system uses an implementation dependent order for efficiency reasons,
sometimes it is important to sort facts by one of the arguments because linearity imposes commitment during rule activation. The syntax for this construct is $[:op => X | ..., \emph{fact}(A, X), ...] \lolli B$. If $op$ is $min$, facts $\emph{fact}(A, X)$ are used in ascending order (by $X$) to prove the rule. Other operations available are $max$ and $random$.

The head of a rule contains \emph{fact templates} which are uninstantiated facts and will derive new facts. The head can also have \emph{exist constructs}, \emph{comprehensions} and \emph{aggregates}. All those constructs may use all the variables instantiated in the body.

Exist constructs are based on the linear logic construct of the same name and are used to create new node addresses. Syntactically they are written as $exists \; A. (..., \emph{fact}(A, ...), ...)$, where $A$ is the new bound variable with type \emph{node}. This variable can then be used inside the exists scope to derive new facts.

Comprehensions are sub-rules that are applied with all possible combinations using the facts from the database. Their syntax is the following: $\{X, ..., Z | A | B\}$, where $X, ..., Z$ are bound variables that are instantiated using the fact templates in $A$, $A$ is the body of the comprehension and $B$ is the head of the comprehension.

Aggregates are a special kind of sub-rule that work very similarly to comprehensions. Their syntax is $[:op => T | X, ..., Z | A | B]$, where $A$ is the body of the aggregate, $X, ..., Z$ are the variables bound by the body, $op$ is the aggregate operation, $T$ is the aggregate variable and $B$ is the aggregate head. To compute the aggregate, we try all the combinations of $A$ using the database. Note that a different $T$ is instantiated for every combination. From all those $T$'s we pick one using $op$, this can be the minimum, maximum, sum, etc. After all the combinations, we fire $B$ once, with $T$ instantiated as the result of the aggregate operation.

\subsection{Operational Semantics}

Each rule in \lang has a defined priority that is inferred from its position in the source file. Rules at the beginning of the file have higher priority. We separate the database into two sets,
the database itself, and the \emph{temporary store}. The temporary store are facts that have been
derived or have been sent to the node but have not been considered in rule derivations.
The temporary store exists for efficiency reasons because many linear facts are short-lived.

Operationally, node execution goes as following. We first consider all the facts in the
temporary store and in the database and pick \emph{candidate rules} that may result in
successful derivations. The candidate rules are then inserted into a priority queue ordered
by the rule's priorities. We take the highest priority rule from the queue and then run it.
If the derivation was successful, new facts may have been derived, thus we need to consider new
rules that are candidates from the newly derived facts and add them to the rule's priority queue.
On the other hand, when linear facts are consumed, some rules may not be applicable anymore and thus
we may need to remove them from the priority queue. In our implementation,
we keep a fact count for each predicate per node and also which predicates are needed for each rule. Whenever we have facts of some
predicates we can efficiently check if new rules are applicable for a specific node. We keep taking rules from the
queue until the queue is empty. However, we do not check for rule constraints.

Node execution can be performed at any time. This means that the programmer cannot expect
that facts coming from other nodes will be considered as a whole or partially.
Usually, it is preferable that rules are written as if rule order didn't matter, although
rule order makes things easier for ordering local computation.

We do a small optimization to reduce the number of derivations of persistent facts. We
divide the program rules into two sets: \emph{persistent rules} and \emph{non persistent rules}.
Persistent rules are rules where only persistent facts are involved. We compile such rules
incrementally, that is, we attempt to fire all rules where a persistent fact is used. This is called
the \emph{pipelined semi-naive} evaluation and it originated in the P2 system~\cite{Loo-condie-garofalakis-p2}.
This evaluation method avoids excessing re-derivations of the same fact. The order of derivation does not matter for those rules, since
only persistent facts are used.